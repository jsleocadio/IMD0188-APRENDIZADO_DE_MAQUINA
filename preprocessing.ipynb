{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/iohans/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in ./glpi_venv/lib/python3.12/site-packages (from pt-core-news-sm==3.7.0) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (74.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./glpi_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in ./glpi_venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./glpi_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./glpi_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./glpi_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./glpi_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./glpi_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./glpi_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./glpi_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./glpi_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./glpi_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./glpi_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./glpi_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./glpi_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (13.8.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./glpi_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./glpi_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./glpi_venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in ./glpi_venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./glpi_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./glpi_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in ./glpi_venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./glpi_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62726 entries, 0 to 62725\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            62726 non-null  int64 \n",
      " 1   name          62725 non-null  object\n",
      " 2   content       62723 non-null  object\n",
      " 3   category      62229 non-null  object\n",
      " 4   fullcategory  62229 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load data from file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullcategory</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Equipamentos de informática (falha/substituiçã...</td>\n",
       "      <td>47</td>\n",
       "      <td>Apenas um teste</td>\n",
       "      <td>apenas um teste</td>\n",
       "      <td>Equipamentos de informática (falha/substituiçã...</td>\n",
       "      <td>Equipamentos de informática (falha/substituiçã...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>48</td>\n",
       "      <td>Desbloqueio de conta de usuária</td>\n",
       "      <td>Solicito o desbloqueio da minha conta para ace...</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>49</td>\n",
       "      <td>IMPOSSIBILIDADE DE USO DO PAE E SRHonline</td>\n",
       "      <td>Bom dia,\\r\\n\\r\\n Estou sem consegui acessar P...</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>50</td>\n",
       "      <td>IMPOSSIBILIDADE DE ENTRAR NO E-MAIL e ATENDIME...</td>\n",
       "      <td>Bom dia,\\r\\n\\r\\n Estava sem conseguir acessar,...</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>51</td>\n",
       "      <td>Desabilitação</td>\n",
       "      <td>Desabilitação de todos os acessos dos e-mails ...</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>Habilitação/Acesso a sistemas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        fullcategory  id  \\\n",
       "0  Equipamentos de informática (falha/substituiçã...  47   \n",
       "1                      Habilitação/Acesso a sistemas  48   \n",
       "2                      Habilitação/Acesso a sistemas  49   \n",
       "3                      Habilitação/Acesso a sistemas  50   \n",
       "4                      Habilitação/Acesso a sistemas  51   \n",
       "\n",
       "                                                name  \\\n",
       "0                                    Apenas um teste   \n",
       "1                    Desbloqueio de conta de usuária   \n",
       "2          IMPOSSIBILIDADE DE USO DO PAE E SRHonline   \n",
       "3  IMPOSSIBILIDADE DE ENTRAR NO E-MAIL e ATENDIME...   \n",
       "4                                      Desabilitação   \n",
       "\n",
       "                                             content  \\\n",
       "0                                    apenas um teste   \n",
       "1  Solicito o desbloqueio da minha conta para ace...   \n",
       "2   Bom dia,\\r\\n\\r\\n Estou sem consegui acessar P...   \n",
       "3  Bom dia,\\r\\n\\r\\n Estava sem conseguir acessar,...   \n",
       "4  Desabilitação de todos os acessos dos e-mails ...   \n",
       "\n",
       "                                            category  \\\n",
       "0  Equipamentos de informática (falha/substituiçã...   \n",
       "1                      Habilitação/Acesso a sistemas   \n",
       "2                      Habilitação/Acesso a sistemas   \n",
       "3                      Habilitação/Acesso a sistemas   \n",
       "4                      Habilitação/Acesso a sistemas   \n",
       "\n",
       "                                           category1 category2 category3  \n",
       "0  Equipamentos de informática (falha/substituiçã...      None      None  \n",
       "1                      Habilitação/Acesso a sistemas      None      None  \n",
       "2                      Habilitação/Acesso a sistemas      None      None  \n",
       "3                      Habilitação/Acesso a sistemas      None      None  \n",
       "4                      Habilitação/Acesso a sistemas      None      None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get all distinct full categories\n",
    "categories = data.drop_duplicates(subset=['fullcategory'])[['category', 'fullcategory']]\n",
    "\n",
    "# Split full categories by level\n",
    "categories_split = categories.join(\n",
    "    categories['fullcategory'].str.split(' > ', expand=True)\n",
    ").rename(columns={0: 'category1', 1: 'category2', 2: 'category3'})[['category1', 'category2', 'category3', 'category', 'fullcategory']]\n",
    "\n",
    "# print(categories_split.drop_duplicates(subset=['category1'])[['category1']].dropna().count())\n",
    "# print(categories_split.drop_duplicates(subset=['category2'])[['category2']].dropna().count())\n",
    "# print(categories_split.drop_duplicates(subset=['category3'])[['category3']].dropna().count())\n",
    "\n",
    "# comparison = categories_split[['category1', 'category2', 'category3', 'category']].copy()\n",
    "\n",
    "# for column in ['category1', 'category2', 'category3']:\n",
    "#     comparison[column] = categories_split[['category',column]].dropna()['category'] == categories_split[column].dropna()\n",
    "\n",
    "# Merge data with split categories\n",
    "data_category = data.set_index('fullcategory').join(categories_split[['fullcategory','category1', 'category2', 'category3']].set_index('fullcategory'))\n",
    "\n",
    "# Reset index\n",
    "data_category = data_category.reset_index()\n",
    "\n",
    "data_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values for category1\n",
    "data_category = data_category.dropna(subset=['category1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_noise(text):\n",
    "#     import re\n",
    "\n",
    "#     # Converte para minúsculas\n",
    "#     text = text.lower()\n",
    "\n",
    "#     # Remove pontuação\n",
    "#     # []: colchetes são usados para definir uma classe de caracteres.\n",
    "#     # ^: quando usado no início de uma classe de caracteres, o ^ nega a classe, ou seja, seleciona tudo que não está na classe.\n",
    "#     # \\w: corresponde a qualquer caractere alfanumérico (letras e números, incluindo o caractere de sublinhado _)\n",
    "#     # \\s: corresponde a qualquer espaço em branco (espaços, tabulações, quebras de linha).\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_clean = ['name','content']\n",
    "\n",
    "# for column in columns_to_clean:\n",
    "#     data[column] = data[column].apply(remove_noise)\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopwords(text):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     # Obtém a lista de stopwords em português usando o NLTK e as converte para um conjunto para melhorar a eficiência da busca\n",
    "#     stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "#     # Divide o texto em palavras, remove as stopwords e então junta as palavras restantes de volta em uma string\n",
    "#     text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in columns_to_clean:\n",
    "#     data[column] = data[column].apply(remove_stopwords)\n",
    "\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Cria um objeto OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# enc_category1 = encoder.fit_transform(data_category[['category1']])\n",
    "\n",
    "# data_category = pd.concat([data_category, pd.DataFrame(enc_category1, columns=encoder.get_feature_names_out(['category1']))], axis=1)\n",
    "\n",
    "# data_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15557 entries, 25531 to 12872\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   name       15557 non-null  object\n",
      " 1   content    15557 non-null  object\n",
      " 2   category1  15557 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 486.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15557 entries, 39663 to 12571\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   name       15556 non-null  object\n",
      " 1   content    15557 non-null  object\n",
      " 2   category1  15557 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 486.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 31115 entries, 49915 to 7510\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   name       31115 non-null  object\n",
      " 1   content    31114 non-null  object\n",
      " 2   category1  31115 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 972.3+ KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(data_category[['name', 'content']], data_category['category1'], test_size=0.5, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_val = pd.concat([X_val, y_val], axis=1)\n",
    "data_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "data_train.info()\n",
    "data_val.info()\n",
    "data_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Instalar', 'Mozila', 'Máquina', 'IP']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy, regex as re\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if \n",
    "        not re.search(r'[^\\w\\s]|[\\d]|[\\w\\-\\.]+@([\\w-]+\\.)+[\\w-]{2,}|[\\r\\n\\t]', token.lemma_) \n",
    "        and \n",
    "        token.is_stop == False\n",
    "        ]\n",
    "    return tokens\n",
    "\n",
    "teste = data['content'].sample(100).iloc[0]\n",
    "\n",
    "tokenize_and_lemmatize(teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importa TfidfVectorizer para criar vetores TF-IDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# save_dir = './vectorizers'\n",
    "\n",
    "# import os\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# vectorizers = {}\n",
    "\n",
    "# for min_df in [0.01, 0.025, 0.05, 0.075, 0.1]:\n",
    "#     # instancia um objeto TfidfVectorizer\n",
    "#     tfidf_vectorizer = TfidfVectorizer(max_df=1.0, max_features=200000,\n",
    "#                                     min_df=min_df,\n",
    "#                                     use_idf=True, tokenizer=tokenize_and_lemmatize,\n",
    "#                                     ngram_range=(1,3))\n",
    "\n",
    "#     # Check if the vectorizer file exists, if so, load it, if not, create it\n",
    "#     if os.path.exists(f'{save_dir}/tfidf_vectorizer_{min_df}.pkl'):\n",
    "#         with open(f'{save_dir}/tfidf_vectorizer_{min_df}.pkl', 'rb') as f:\n",
    "#             print(f'Loading tfidf_vectorizer_{min_df}')\n",
    "#             import pickle\n",
    "#             tfidf_vectorizer = pickle.load(f)\n",
    "#     else:\n",
    "#         print(f'Creating tfidf_vectorizer_{min_df}')\n",
    "#         # Create the vectorizer\n",
    "#         tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in data_train[['name', 'content']].apply(lambda x: ' '.join(x), axis=1)])\n",
    "\n",
    "#         print(tfidf_matrix.shape)\n",
    "\n",
    "#         print(f'min_df={min_df}: {len(tfidf_vectorizer.get_feature_names_out())} features')\n",
    "\n",
    "#         # save the model to disk\n",
    "#         # from joblib import dump\n",
    "#         import pickle\n",
    "#         with open(f'{save_dir}/tfidf_vectorizer_{min_df}.pkl', 'wb') as f:\n",
    "#             pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "\n",
    "#     vectorizers.update({f'tfidf_vectorizer_{min_df}': tfidf_vectorizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tfidf_vectorizer_0.01\n",
      "min_df=0.01: 377 features\n",
      "Loading tfidf_vectorizer_0.025\n",
      "min_df=0.025: 138 features\n",
      "Loading tfidf_vectorizer_0.05\n",
      "min_df=0.05: 54 features\n",
      "Loading tfidf_vectorizer_0.075\n",
      "min_df=0.075: 30 features\n",
      "Loading tfidf_vectorizer_0.1\n",
      "min_df=0.1: 19 features\n"
     ]
    }
   ],
   "source": [
    "# importa TfidfVectorizer para criar vetores TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "save_dir = './vectorizers'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "vectorizers = {}\n",
    "\n",
    "def create_or_load_vectorizer(min_df, data_train, save_dir, max_features=2000):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    # instancia um objeto TfidfVectorizer\n",
    "    if min_df is None:\n",
    "      tfidf_vectorizer = TfidfVectorizer(max_df=1.0,\n",
    "                                        max_features=max_features,\n",
    "                                        use_idf=True, tokenizer=tokenize_and_lemmatize,\n",
    "                                        ngram_range=(1, 3))\n",
    "    else:\n",
    "      tfidf_vectorizer = TfidfVectorizer(max_df=1.0,\n",
    "                                        max_features=max_features,\n",
    "                                        min_df=min_df,\n",
    "                                        use_idf=True, tokenizer=tokenize_and_lemmatize,\n",
    "                                        ngram_range=(1, 3))\n",
    "\n",
    "    # Check if the vectorizer file exists, if so, load it, if not, create it\n",
    "    if os.path.exists(f'{save_dir}/tfidf_vectorizer_{min_df}.pkl'):\n",
    "        with open(f'{save_dir}/tfidf_vectorizer_{min_df}.pkl', 'rb') as f:\n",
    "            print(f'Loading tfidf_vectorizer_{min_df}')\n",
    "            tfidf_vectorizer = pickle.load(f)\n",
    "            print(f'min_df={min_df}: {len(tfidf_vectorizer.get_feature_names_out())} features')\n",
    "    else:\n",
    "        print(f'Creating tfidf_vectorizer_{min_df}')\n",
    "        # Create the vectorizer\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in data_train[['name', 'content']].apply(lambda x: ' '.join(x), axis=1)])\n",
    "\n",
    "        print(tfidf_matrix.shape)\n",
    "        print(f'min_df={min_df}: {len(tfidf_vectorizer.get_feature_names_out())} features')\n",
    "\n",
    "        # save the model to disk\n",
    "        with open(f'{save_dir}/tfidf_vectorizer_{min_df}.pkl', 'wb') as f:\n",
    "            pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "    return f'tfidf_vectorizer_{min_df}', tfidf_vectorizer\n",
    "\n",
    "min_dfs = [0.01, 0.025, 0.05, 0.075, 0.1]\n",
    "\n",
    "# Use ProcessPoolExecutor to parallelize the creation/loading of vectorizers\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(create_or_load_vectorizer, min_df, data_train, save_dir) for min_df in min_dfs]\n",
    "\n",
    "    # Collect results\n",
    "    for future in futures:\n",
    "        vectorizer_name, vectorizer = future.result()\n",
    "        vectorizers[vectorizer_name] = vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and create feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(data, vectorizer, columns, feature_names=False):\n",
    "    # Join the columns into a single column\n",
    "    temp = data[columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "    # Transform the column using the vectorizer\n",
    "    tfidf_matrix = temp.apply(lambda x: vectorizer.transform([x]).toarray()[0])\n",
    "\n",
    "    # Convert the list of arrays to a DataFrame\n",
    "    if feature_names:\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.tolist(), index=tfidf_matrix.index, columns=vectorizer.get_feature_names_out())\n",
    "    else:\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.tolist(), index=tfidf_matrix.index)\n",
    "\n",
    "    # Create a DataFrame with the features\n",
    "    feature_columns = pd.DataFrame(tfidf_df, index=tfidf_df.index)\n",
    "\n",
    "    return feature_columns\n",
    "\n",
    "def get_vectorized_df(data, vectorizer, label_column, text_columns, feature_names=False):\n",
    "    feature_columns = get_feature_columns(data.dropna(), vectorizer, text_columns, feature_names)\n",
    "\n",
    "    # Merge the label column with the feature columns\n",
    "    vectorized_df = data[label_column].to_frame().merge(feature_columns, left_index=True, right_index=True)\n",
    "\n",
    "    return vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the feature columns for the training data\n",
    "# datasets = {\n",
    "#     'train': data_train,\n",
    "#     'val': data_val,\n",
    "# }\n",
    "\n",
    "# save_dir = './vectorized_data'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# vectorized_dfs = {}\n",
    "\n",
    "# for vectorizer in vectorizers.keys():\n",
    "#     for dataset in datasets.keys():\n",
    "#         file_path = f'{save_dir}/{vectorizer}_{dataset}.csv'\n",
    "#         if(f'{vectorizer}_{dataset}.csv' in os.listdir('./vectorized_data')):\n",
    "#             print(f'Loading {dataset} with {vectorizer}')\n",
    "#             vectorized_dfs.update({f'{vectorizer}': \n",
    "#                                 {f'{dataset}': pd.read_csv(file_path)}\n",
    "#                                 })\n",
    "#         else:\n",
    "#             print(f'Processing {dataset} with {vectorizer}')\n",
    "#             vectorized_dfs.update({f'{vectorizer}': \n",
    "#                                 {f'{dataset}': get_vectorized_df(datasets[dataset], vectorizers[vectorizer], 'category1', ['name', 'content'], feature_names=True)}\n",
    "#                                 })\n",
    "#             vectorized_dfs[vectorizer][dataset].to_csv(f'./vectorized_data/{vectorizer}_{dataset}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train with tfidf_vectorizer_0.01\n",
      "Loading val with tfidf_vectorizer_0.01\n",
      "Loading train with tfidf_vectorizer_0.025\n",
      "Loading val with tfidf_vectorizer_0.025\n",
      "Loading train with tfidf_vectorizer_0.05\n",
      "Loading val with tfidf_vectorizer_0.05\n",
      "Loading train with tfidf_vectorizer_0.075\n",
      "Loading val with tfidf_vectorizer_0.075\n",
      "Loading train with tfidf_vectorizer_0.1\n",
      "Loading val with tfidf_vectorizer_0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "datasets = {\n",
    "    'train': data_train,\n",
    "    'val': data_val,\n",
    "}\n",
    "\n",
    "save_dir = './vectorized_data'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_vectorizer_dataset(vectorizer, dataset, save_dir, datasets, vectorizers):\n",
    "    file_path = f'{save_dir}/{vectorizer}_{dataset}.csv'\n",
    "    vectorized_dfs = {}\n",
    "    if f'{vectorizer}_{dataset}.csv' in os.listdir('./vectorized_data'):\n",
    "        print(f'Loading {dataset} with {vectorizer}')\n",
    "        vectorized_dfs.update({f'{vectorizer}': \n",
    "                            {f'{dataset}': pd.read_csv(file_path)}\n",
    "                            })\n",
    "    else:\n",
    "        print(f'Processing {dataset} with {vectorizer}')\n",
    "        vectorized_dfs.update({f'{vectorizer}': \n",
    "                            {f'{dataset}': get_vectorized_df(datasets[dataset], vectorizers[vectorizer], 'category1', ['name', 'content'], feature_names=True)}\n",
    "                            })\n",
    "        vectorized_dfs[vectorizer][dataset].to_csv(f'./vectorized_data/{vectorizer}_{dataset}.csv', index=True)\n",
    "    return vectorized_dfs\n",
    "\n",
    "# Create a list of all combinations of vectorizers and datasets\n",
    "tasks = [(vectorizer, dataset) for vectorizer in vectorizers.keys() for dataset in datasets.keys()]\n",
    "\n",
    "# Use ProcessPoolExecutor to parallelize the processing\n",
    "vectorized_dfs = {}\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_vectorizer_dataset, vectorizer, dataset, save_dir, datasets, vectorizers) for vectorizer, dataset in tasks]\n",
    "\n",
    "    # Collect results\n",
    "    for future in futures:\n",
    "        result = future.result()\n",
    "        for vectorizer, datasets in result.items():\n",
    "            if vectorizer not in vectorized_dfs:\n",
    "                vectorized_dfs[vectorizer] = {}\n",
    "            vectorized_dfs[vectorizer].update(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tfidf_vectorizer_0.01', 'train'), ('tfidf_vectorizer_0.01', 'val'), ('tfidf_vectorizer_0.025', 'train'), ('tfidf_vectorizer_0.025', 'val'), ('tfidf_vectorizer_0.05', 'train'), ('tfidf_vectorizer_0.05', 'val'), ('tfidf_vectorizer_0.075', 'train'), ('tfidf_vectorizer_0.075', 'val'), ('tfidf_vectorizer_0.1', 'train'), ('tfidf_vectorizer_0.1', 'val')]\n",
      "Loading train with tfidf_vectorizer_0.01\n",
      "Loading val with tfidf_vectorizer_0.01\n",
      "Loading train with tfidf_vectorizer_0.025\n",
      "Loading val with tfidf_vectorizer_0.025\n",
      "Loading train with tfidf_vectorizer_0.05\n",
      "Loading val with tfidf_vectorizer_0.05\n",
      "Loading train with tfidf_vectorizer_0.075\n",
      "Loading val with tfidf_vectorizer_0.075\n",
      "Loading train with tfidf_vectorizer_0.1\n",
      "Loading val with tfidf_vectorizer_0.1\n"
     ]
    }
   ],
   "source": [
    "print(tasks)\n",
    "\n",
    "for vectorizer, dataset in tasks:\n",
    "    process_vectorizer_dataset( vectorizer, dataset, save_dir, datasets, vectorizers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glpi_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
